{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7154f2b2-29ad-42aa-813e-0536b9a4c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# Ridge Regression \n",
    "# It is a type Regression which helps to reduce overfitting of a model\n",
    "# Ridge regression shrinks the coefficient estimates towards zero. \n",
    "# However, it does not force any coefficients to become exactly zero unless the regularization parameter (Î») is set to an extremely large value.\n",
    "# It introducs a penalty term to the cost  function. \n",
    "\n",
    "# In ordinary least squares (OLS) regression, the goal is to minimize the sum of squared residuals between the predicted values and \n",
    "# the actual values of the response variable\n",
    "# But when it comes to multicollinearity Ridge regression handles it \n",
    "# Ridge regression addresses this problem by adding a penalty term to the OLS cost function\n",
    "\n",
    "# major difference is \n",
    "# 1.Handling multicollinearity: Ridge regression is particularly effective in handling multicollinearity by \n",
    "# shrinking the coefficient estimates of correlated predictors together\n",
    "# where OLS regression does not explicitly address multicollinearity and \n",
    "# may produce unstable or unreliable coefficient estimates in the presence of highly correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f422cf-c161-4cba-9f72-d43e58bae4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# Assumptions\n",
    "# 1.Linearity: Ridge regression assumes a linear relationship between the predictors and the response variable.\n",
    "# 2.Independence: The observations used in Ridge regression should be independent of each other.\n",
    "# 3.Normality of residuals: Ridge regression assumes that the residuals follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de5b09a-008c-483e-b37a-18a4da0a85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3:\n",
    "# Ridge regression involves a process called regularization parameter tuning.\n",
    "# The goal is to find the value of lambda that provides the best balance between model simplicity (shrinking coefficients towards zero) and predictive performance.\n",
    "\n",
    "# we can select in multiple ways\n",
    "# 1.Grid Search: In grid search, a predefined set of lambda values is specified, and Ridge regression is performed for each value. \n",
    "# 2.Cross-Validation: Cross-validation is a widely used technique for model evaluation and parameter selection.\n",
    "#     The dataset is divided into multiple subsets or folds, and Ridge regression is performed on different combinations of training and validation sets.\n",
    "# 3.Bayesian Optimization: Bayesian optimization is a sequential model-based optimization technique that searches for the \n",
    "#    optimal value of lambda by iteratively evaluating the performance of Ridge regression for different lambda values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4dc3f7-de5d-4a6d-96f2-2e3a178dff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# Ridge regression, unlike Lasso regression, is not typically used for explicit feature selection because \n",
    "# it does not drive coefficients to exactly zero\n",
    "\n",
    "# However, Ridge regression can still indirectly provide some level of feature selection by shrinking the coefficients towards zero and \n",
    "# reducing the impact of less important predictors.\n",
    "\n",
    "# It can use different echniques\n",
    "# 1.Ridge with Cross-Validation: By performing Ridge regression with cross-validation, you can assess the impact of different lambda values on the performance of the model. \n",
    "#  Higher values of lambda will shrink the coefficients more aggressively, leading to more implicit feature selection.\n",
    "# 2.Hybrid Approaches: You can also consider hybrid approaches that combine Ridge regression with other feature selection methods. \n",
    "#   For example, you can apply an initial feature selection technique (e.g., univariate selection, correlation analysis) to narrow down the set of predictors, \n",
    "#   and then perform Ridge regression on the selected subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab25948-aef3-4813-8a70-f39a78d28756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# Ridge regression is particularly effective in handling multicollinearity, which refers to high correlation among predictor variables.\n",
    "\n",
    "#  Ridge regression performs in the presence of multicollinearity:\n",
    "\n",
    "# 1.Shrinkage of coefficient estimates: Ridge regression shrinks the coefficient estimates towards zero, reducing their magnitudes.\n",
    "#   By shrinking the coefficients, Ridge regression reduces the amplification of small changes in the data due to multicollinearity.\n",
    "#  This results in more stable and reliable coefficient estimates compared to OLS regression.\n",
    "# 2.Relative importance of predictors: Ridge regression handles multicollinearity by shrinking correlated predictors together. \n",
    "#   It does not force any coefficients to become exactly zero unless the regularization parameter (lambda) is set to an extremely large value.\n",
    "# 3.Improved prediction accuracy: By reducing the impact of multicollinearity, Ridge regression improves the prediction accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc78ab30-6644-460a-9c8b-6de2aa6a5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6:\n",
    "# Ridge can handle only continous or numerical data but it can handle categorical data if it is converted into numerical\n",
    "\n",
    "# 1.Categorical Variables: Categorical variables need to be converted into a numerical format for use in Ridge regression. \n",
    "#  One common approach is to use one-hot encoding or dummy variable encoding.\n",
    "# 2.Continuous Variables: Ridge regression naturally handles continuous independent variables without requiring any specific transformation. \n",
    "#   The coefficients of the continuous predictors are estimated directly using the Ridge regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0467fa5f-dbfd-41e9-8b9a-0f92bc48c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# we can interpret the coefficient of ridge regression\n",
    "# 1.Magnitude: The magnitude of the coefficient estimates in Ridge regression represents the strength of the relationship between \n",
    "#   the predictor variable and the response variable, similar to OLS regression\n",
    "# 2.Sign: The sign of the coefficient indicates the direction of the relationship between the predictor and the response\n",
    "# 3.Comparison within the Model: When interpreting the coefficients in Ridge regression, it's useful to compare the magnitudes and signs of the coefficients within the same model. \n",
    "#  This comparison helps understand the relative importance and direction of the relationships among the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441eb98-7116-4cfe-86ec-169d167f0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8 :\n",
    "# Ridge regression can be used for time-series data analysis, although it requires some considerations\n",
    "# Here are some ways Ridge regression can be applied to time-series data:\n",
    "# 1.Lagged Variables: In time-series analysis, it is common to include lagged versions of the response variable or predictors as additional features.\n",
    "#   By incorporating lagged variables into the regression model, Ridge regression can capture the temporal dependencies and patterns in the data.\n",
    "# 2.Autocorrelation Structure: Time-series data often exhibit autocorrelation, where the observations are correlated with their past values.\n",
    "# 3.Rolling Windows: When applying Ridge regression to time-series data, \n",
    "#    it is important to account for the temporal nature of the data and potential changes in relationships over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
